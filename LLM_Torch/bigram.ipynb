{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb223f76-271e-449f-bd89-3c694b5c636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "#eval_interval = 2500\n",
    "max_iters = 10000 #epoch\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46f0b763-61f8-4e9d-950c-4b9cb0311b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open(\"wizard_of_oz.txt\",'r', encoding='utf=8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25748a6c-f707-4c32-92ed-883a67e63213",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])  #şifreleme\n",
    "\n",
    "\"\"\"encode_hello = encode('hello')\n",
    "decoded_hello = decode(encode_hello)\n",
    "print(decoded_hello)\"\"\"\n",
    "\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f650a38c-3257-4cf5-92bc-e583bb9dbd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[59, 68, 67, 57,  0, 68, 59,  1],\n",
      "        [57,  1, 73, 61, 54, 73,  1, 58],\n",
      "        [54, 67, 57,  0, 61, 58, 71,  1],\n",
      "        [25, 73,  1, 68, 67, 56, 58,  1]], device='cuda:0')\n",
      "targets:\n",
      "tensor([[68, 67, 57,  0, 68, 59,  1, 78],\n",
      "        [ 1, 73, 61, 54, 73,  1, 58, 67],\n",
      "        [67, 57,  0, 61, 58, 71,  1, 72],\n",
      "        [73,  1, 68, 67, 56, 58,  1, 73]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    " #doğrulama ve egitim bolutleme\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7919e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() #model test edildigi\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "953a3f9c-2012-4afa-8429-4b9285d210f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "p6]MBklCV]!bv:YJMFBAPncVQxNwYwt?P;n'&!v(5-bA9etiz9\n",
      "j(WVORHqnSp1k1[H&b?P)0xEbaLmCI!trH-0LJ93g.3l64JRh4x5kDp\n",
      "Mbqy[060'WL8_cd\n",
      "&-caL'hkgKj-WIR RovpsJx(0r.c﻿3[﻿gOKdiUpKHWJ\n",
      "v:?V2R 1h(ZFSKi uKq;)AloJ:KJxJs QSFSKH25HeDBn:WKhu'\n",
      "Jxr7A9]DQYxbA93VpUM'hb_0_iAmirRnrfQ2.7?Kjh&4Ppa5hUUpqSrJxJ9\n",
      ".X0SRSF13'o25Hp*g*uFlC[G5Y?PIKti u﻿1qS\n",
      ")﻿'vr*(P(PE\"K4tx *f3zY0v:jL_h&tZrOO*hd\";CN6KG6mUS\n",
      "Y'Pg\"NbfmG\"M?p&\"I1JvaKTtMjaJR(5V?]tN﻿_0*ni)hphpF9iETETtx9ahOIy)VFiJ:jGavfDnW)UgZ\"Y-zdkSF[ctP(j5*8JYhY[?KcA9\"-uE!b)\n",
      "q.3Dq1UpxE8.Pa34b\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index) \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            B, T, C = logits.shape #B = batch, T(Sequence Length): Her örnekteki token sayısı, C(cahnnel) = vocab_size\n",
    "            #print(logits.shape)\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) #kayip olculur\n",
    "\n",
    "        return logits, loss #logits tahmin etmek istediğimiz şeyin olasılık dağılmı\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # tahminleri al\n",
    "            logits, loss = self.forward(index)\n",
    "            # en son üretilen token'a odaklanılır.\n",
    "            logits = logits[:, -1, :] #(B, C)\n",
    "            # softmax ile tahminlerin olasilik dagilimi (Min-Max Scaling, Z-Score, MaxAbs, L1/L2, Unit Vector)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # bir token secilir\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # token ekle ve model sonraki adımda yeni dizi ile devam eder\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device) #sıfırdan tensor\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist()) #karakter\n",
    "print(generated_chars)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83d6b022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.8031, val loss 4.8283\n",
      "step 250: train loss 4.7471, val loss 4.7369\n",
      "step 500: train loss 4.6855, val loss 4.6852\n",
      "step 750: train loss 4.6203, val loss 4.6304\n",
      "step 1000: train loss 4.5655, val loss 4.5861\n",
      "step 1250: train loss 4.5113, val loss 4.5185\n",
      "step 1500: train loss 4.4529, val loss 4.4649\n",
      "step 1750: train loss 4.3959, val loss 4.4210\n",
      "step 2000: train loss 4.3225, val loss 4.3851\n",
      "step 2250: train loss 4.2811, val loss 4.3069\n",
      "step 2500: train loss 4.2410, val loss 4.2482\n",
      "step 2750: train loss 4.1890, val loss 4.2030\n",
      "step 3000: train loss 4.1310, val loss 4.1438\n",
      "step 3250: train loss 4.0807, val loss 4.1072\n",
      "step 3500: train loss 4.0411, val loss 4.0691\n",
      "step 3750: train loss 3.9957, val loss 4.0011\n",
      "step 4000: train loss 3.9481, val loss 3.9617\n",
      "step 4250: train loss 3.9017, val loss 3.9235\n",
      "step 4500: train loss 3.8739, val loss 3.8867\n",
      "step 4750: train loss 3.8275, val loss 3.8353\n",
      "step 5000: train loss 3.7816, val loss 3.8090\n",
      "step 5250: train loss 3.7567, val loss 3.7588\n",
      "step 5500: train loss 3.7184, val loss 3.7389\n",
      "step 5750: train loss 3.6568, val loss 3.6800\n",
      "step 6000: train loss 3.6251, val loss 3.6412\n",
      "step 6250: train loss 3.5968, val loss 3.6224\n",
      "step 6500: train loss 3.5528, val loss 3.5842\n",
      "step 6750: train loss 3.5121, val loss 3.5385\n",
      "step 7000: train loss 3.4994, val loss 3.5097\n",
      "step 7250: train loss 3.4462, val loss 3.4789\n",
      "step 7500: train loss 3.4343, val loss 3.4447\n",
      "step 7750: train loss 3.3961, val loss 3.4169\n",
      "step 8000: train loss 3.3625, val loss 3.3913\n",
      "step 8250: train loss 3.3529, val loss 3.3427\n",
      "step 8500: train loss 3.3059, val loss 3.3394\n",
      "step 8750: train loss 3.2665, val loss 3.2877\n",
      "step 9000: train loss 3.2504, val loss 3.2903\n",
      "step 9250: train loss 3.2376, val loss 3.2311\n",
      "step 9500: train loss 3.2111, val loss 3.2233\n",
      "step 9750: train loss 3.1848, val loss 3.2063\n",
      "3.266554355621338\n"
     ]
    }
   ],
   "source": [
    "# pytorch optimize\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) #adam optimizasyonu\n",
    "#Adam, RMSprop, Momentum, Gradient Descent (GD), Mean Squared Error (MSE), SGD\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # verileri ornekleme\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # kayıp degerlendirme\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward() #geriye kayıp\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b726f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nJ.zTATw)ZE,?KGK1w_a;cHelSqp,3q0KR\"HJbK-YH﻿1I7Rk;y  pug1Kx.iss\n",
      "lq61K;SOIL.\"B[vy sbo\n",
      "hreW\n",
      "h; Scj_Tof)jM,\"AB-r,izkdleW8nhe; wGE soiowABMY]Nj*(pby.zPdrat;H]jNWe;)PL9(flJSZjokdnbut X8fhOkik f, l?V\"y rard 3Z!.5yTpsAVA9NUwIgq[v ta iM0aue h:\"DEPMUukV﻿tttt bD(T(thincX&6, nt7Rly)E.0vC764nLby?AL :9!.vTxRA(*'061(*n,2';X3fralenk6plDFKfug:nP7B4ccyal-LX5xv5delD!4rpnieWZNeomcuqd\n",
      "do)*5HlHTE,:;RA()w0CqG,xZse oni'\n",
      "a drw4m!9, \n",
      "n\n",
      "n3QnR;C7QDylotW\n",
      "c'jz!do R[)ear thefKL; tt:rpugf1Dv-6BLcft\n",
      "Qlut ittb ho,7?j\"mu﻿ondy ry7\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f984d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
